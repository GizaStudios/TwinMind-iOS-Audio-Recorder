//
//  Models.swift
//  TwinMind
//
//  Created by Devin Morgan on 7/1/25.
//

import Foundation
import SwiftData

// MARK: - Enumerations

/// Where a transcription came from—useful for analytics and fallback logic.
enum TranscriptionSource: String, Codable {
    case whisperAPI
    case appleLocal
    case localWhisper
}

/// Current processing state for a segment's transcription.
enum TranscriptionStatus: String, Codable {
    case notStarted
    case inProgress
    case completed
    case failed
}

// MARK: - RecordingSession

/// Represents a user‑initiated recording session (one tap on "Record").
@Model
class RecordingSession {
    // Primary key
    @Attribute(.unique) var id: UUID

    // User‑visible title or autogenerated timestamp string
    var title: String

    // Session start time
    var createdAt: Date

    // Whole‑session length in seconds (updated when recording stops)
    var duration: TimeInterval

    // Absolute path to the combined/raw audio file on disk
    var audioFilePath: String

    // Audio quality metadata captured at record time
    var sampleRate: Double
    var bitDepth: Int
    var format: String // e.g. "m4a", "wav"

    // Relationship → constituent 30‑s segments
    var segments: [AudioSegment] = []

    init(title: String,
         createdAt: Date = .now,
         duration: TimeInterval = 0,
         audioFilePath: String,
         sampleRate: Double,
         bitDepth: Int,
         format: String) {
        self.id = UUID()
        self.title = title
        self.createdAt = createdAt
        self.duration = duration
        self.audioFilePath = audioFilePath
        self.sampleRate = sampleRate
        self.bitDepth = bitDepth
        self.format = format
    }
}

// MARK: - AudioSegment

/// A slice of the recording (default 30 s) that is independently transcribed.
@Model
class AudioSegment {
    @Attribute(.unique) var id: UUID

    // Offset relative to RecordingSession start
    var startTime: TimeInterval
    var endTime: TimeInterval

    // Path to the on‑disk audio slice
    var segmentFilePath: String

    // Transcription pipeline state
    var status: TranscriptionStatus
    var retryCount: Int
    var lastError: String?

    // Relationships
    var transcription: Transcription?
    var session: RecordingSession?

    init(startTime: TimeInterval,
         endTime: TimeInterval,
         segmentFilePath: String,
         status: TranscriptionStatus = .notStarted,
         retryCount: Int = 0,
         lastError: String? = nil) {
        self.id = UUID()
        self.startTime = startTime
        self.endTime = endTime
        self.segmentFilePath = segmentFilePath
        self.status = status
        self.retryCount = retryCount
        self.lastError = lastError
    }
}

// MARK: - Transcription

/// Holds the text produced for an AudioSegment plus metadata.
@Model
class Transcription {
    @Attribute(.unique) var id: UUID

    var text: String
    var confidence: Double // 0.0 – 1.0
    var language: String?  // ISO 639‑1 code if available
    var createdAt: Date
    var source: TranscriptionSource

    // Back‑reference
    var segment: AudioSegment?

    init(text: String,
         confidence: Double,
         language: String? = nil,
         createdAt: Date = .now,
         source: TranscriptionSource) {
        self.id = UUID()
        self.text = text
        self.confidence = confidence
        self.language = language
        self.createdAt = createdAt
        self.source = source
    }
} 